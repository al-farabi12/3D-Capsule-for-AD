{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storona = 30\n",
    "dims = (storona,storona,storona)\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self,channels=[1,256],kernel_size=[9],stride=[1],print_=False):\n",
    "        super(ConvLayer,self).__init__()\n",
    "        self.layers=len(channels)-1\n",
    "        self.print_=print_\n",
    "        self.conv = nn.ModuleList([torch.nn.Conv3d(channels[l],channels[l+1],kernel_size[l],stride[l])\n",
    "                                   for l in range(self.layers)])\n",
    "#         self.conv = torch.nn.Conv3d(channels[0],channels[-1],kernel_size[0],stride[0])\n",
    "#         for l in range(layers):\n",
    "#             self.conv[l] = torch.nn.Conv3d(in_ch,out_ch[l],kernel_size[l],stride[l])\n",
    "    def setPrint(print_):\n",
    "        self.print_=print_\n",
    "    def squash(self,x):\n",
    "        if self.print_:\n",
    "            print(\"shape before squash \",x.shape)\n",
    "        norms = torch.norm(x,dim=1,keepdim=True)\n",
    "#         print(norms.shape)\n",
    "        x = ((norms/(1+norms**2))*x)\n",
    "        if self.print_:\n",
    "            print(\"norms.shape\", norms.shape)\n",
    "            print(\"shape after squash \",x.shape)\n",
    "\n",
    "        return x\n",
    "    def print_size(self,f):\n",
    "        self.print_=f\n",
    "    def forward(self,x):\n",
    "        x = F.adaptive_avg_pool3d(x,dims)\n",
    "#         x = F.avg_pool3d(x,3)\n",
    "#         x = F.max_pool3d(x,3)\n",
    "        for l in range(self.layers):\n",
    "            print('Layer',l)\n",
    "            if self.print_:\n",
    "                print(f\"shape before conv[{l}] \",x.shape)\n",
    "            x = self.conv[l](x)\n",
    "            if self.print_:\n",
    "                print(f\"shape after conv[{l}]\",x.shape)\n",
    "            x = self.squash(x)\n",
    "#             x = F.leaky_relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvLayer(channels=[1,256,128],kernel_size=[9,9],stride=[1,1],print_=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    print(i[0].shape)\n",
    "    output = model(i[0])\n",
    "    print(\"final output.shape\",output.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PrimaryCapsules(nn.Module):\n",
    "    def __init__(self,in_ch=256,out_caps_grids=32,out_ch_of_each_caps=8,kernel_size=9,stride=2):\n",
    "        super(PrimaryCapsules,self).__init__()\n",
    "        self.out_caps_grids=out_caps_grids\n",
    "        self.out_ch_of_each_caps=out_ch_of_each_caps\n",
    "        self.prim_caps = torch.nn.Conv3d(in_ch,out_caps_grids*out_ch_of_each_caps,kernel_size,stride)\n",
    "    def reshape(self,x):\n",
    "#         print('reshape prim')\n",
    "        b,ch1,d1,d2,d3=x.shape\n",
    "#         print(x.shape)\n",
    "        x=x.permute(0,2,3,4,1)\n",
    "#         print(x.shape)\n",
    "        x=x.reshape(b,d1,d2,d3,self.out_caps_grids*self.out_ch_of_each_caps)\n",
    "#         print(x.shape)\n",
    "        return x\n",
    "    def squash(self,x):\n",
    "#         print('squash prim')\n",
    "#         print(x.shape)\n",
    "        norms = torch.norm(x,dim=-1,keepdim=True)\n",
    "#         print(norms.shape)\n",
    "        x = ((norms/(1+norms**2))*x)\n",
    "#         print(x.shape)\n",
    "        return x\n",
    "    def forward(self,x):\n",
    "        return self.squash(self.reshape(self.prim_caps(x)))\n",
    "    \n",
    "class DigitCaps(nn.Module):\n",
    "    def __init__(self,input_grid=(6, 6, 6, 32),in_cap_dim=8,out_cap_dim=16,out_caps=3):\n",
    "        super(DigitCaps,self).__init__()\n",
    "        self.in_cap_dim=in_cap_dim\n",
    "        self.num_caps = 1\n",
    "        \n",
    "        for i in input_grid:\n",
    "            self.num_caps*=i\n",
    "        self.W = torch.nn.Parameter(torch.randn(1, out_caps, self.num_caps, out_cap_dim, in_cap_dim))\n",
    "        self.b = torch.nn.Parameter(torch.zeros(1, out_caps, self.num_caps,1,1),requires_grad=False)\n",
    "    def reshape(self,x):\n",
    "#         print('reshape digit')\n",
    "#         print(x.shape)\n",
    "        batch_size=x.shape[0]\n",
    "        x=x.reshape(batch_size,1,-1,self.in_cap_dim,1)\n",
    "        return x\n",
    "    def squash(self,x):\n",
    "        norms = torch.norm(x,dim=-1,keepdim=True)\n",
    "        x = ((norms/(1+norms**2))*x)\n",
    "        return x\n",
    "    def forward(self,x):\n",
    "#         print('forward Digit')\n",
    "        batch_size=x.shape[0]\n",
    "#         print('x',x.shape)\n",
    "        u = self.reshape(x)\n",
    "#         print('w',self.W.shape)\n",
    "#         print('u',u.shape)\n",
    "        \n",
    "        uhat = torch.matmul(self.W,u)\n",
    "#         print('uhat',uhat.shape)\n",
    "        b = self.b\n",
    "\n",
    "        for i in range(3):\n",
    "#             print('iter ',i)\n",
    "#             print('b',self.b.shape)\n",
    "            c = torch.softmax(b,dim=2)\n",
    "#             print('c',c.shape)\n",
    "#             print(c.sum())\n",
    "#             try:\n",
    "            ahat= (uhat*c)\n",
    "#             except:\n",
    "#                 print('b',self.b.shape)\n",
    "\n",
    "#                 print('uhat',uhat.shape)\n",
    "#                 print('c',c.shape)\n",
    "\n",
    "#             print('ahat',ahat.shape)\n",
    "            a=ahat.sum(dim=2,keepdim=True).permute(0,1,2,4,3)\n",
    "#             print('a',a.shape)\n",
    "#             print('uhat',uhat.shape)\n",
    "            adotuhat=torch.matmul(a,uhat).mean(dim=0,keepdim=True)\n",
    "#             print('adotuhat.shape',adotuhat.shape)\n",
    "#             print(\"adotuhat.mean(dim=0,keepdim=True).shape\",adotuhat.mean(dim=0,keepdim=True).shape)\n",
    "#             print(\"b.shape\",b.shape)\n",
    "            b = b+adotuhat\n",
    "            if self.training:\n",
    "                self.b=torch.nn.Parameter(b,requires_grad=False)\n",
    "        x = self.squash(a).squeeze().squeeze()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_id, (data, target) in enumerate(train_loader,1):\n",
    "    data = data.cuda()\n",
    "    print(data.shape)\n",
    "    input_shape=data.shape\n",
    "\n",
    "#!!!!!!!!!!!!!!!!!!!!\n",
    "    out_ch_CONV=[1,128]\n",
    "    kernel_size_CONV=[15]\n",
    "    stride_CONV=[1]\n",
    "\n",
    "    \n",
    "    model1 = ConvLayer(\n",
    "        channels=out_ch_CONV,\n",
    "        kernel_size=kernel_size_CONV,\n",
    "        stride=stride_CONV,\n",
    "        print_=True\n",
    "    ).cuda()\n",
    "    data = model1(data)\n",
    "#     print(data.shape)\n",
    "    model1.print_size(False)\n",
    "\n",
    "#!!!!!!!!!!!!!!!!!!!!\n",
    "    in_ch_PRIM=out_ch_CONV[-1]\n",
    "    out_caps_grids_PRIM=100\n",
    "    out_ch_of_each_caps_PRIM=5\n",
    "    kernel_size_PRIM=7\n",
    "    stride_PRIM=3\n",
    "    model2 = PrimaryCapsules(\n",
    "        in_ch=in_ch_PRIM,\n",
    "        out_caps_grids=out_caps_grids_PRIM,\n",
    "        out_ch_of_each_caps=out_ch_of_each_caps_PRIM,\n",
    "        kernel_size=kernel_size_PRIM,\n",
    "        stride=stride_PRIM\n",
    "    ).cuda()\n",
    "    \n",
    "    data = model2(data)\n",
    "    print(data.shape)\n",
    "    \n",
    "#!!!!!!!!!!!!!!!!!!!!\n",
    "    input_grid_DIGIT=data.shape[1:-1]\n",
    "    in_cap_dim_DIGIT=data.shape[-1]\n",
    "    out_cap_dim_DIGIT=1\n",
    "    out_caps_DIGIT=len(set(diagnos.values()))\n",
    "    model3 = DigitCaps(\n",
    "        input_grid=input_grid_DIGIT,\n",
    "        in_cap_dim=in_cap_dim_DIGIT,\n",
    "        out_cap_dim=out_cap_dim_DIGIT,\n",
    "        out_caps=out_caps_DIGIT\n",
    "    ).cuda()\n",
    "    \n",
    "    data = model3(data)\n",
    "    print(data.shape)\n",
    "#     print(torch.norm(data,dim=-1,keepdim=True).squeeze().squeeze().squeeze().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    model1,\n",
    "    model2,\n",
    "    model3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=0\n",
    "for i,model in enumerate(models):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    print('Number of parameters in model ',i,params)\n",
    "    total+=params\n",
    "print('Number of total parameters model',total) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder,self).__init__()\n",
    "#         self.l1=ConvLayer()\n",
    "#         self.l2=PrimaryCapsules()\n",
    "#         self.l3=DigitCaps()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.model3 = model3\n",
    "#         self.model4 = model4\n",
    "    def forward(self,data):\n",
    "        data = data.cuda()\n",
    "        data = self.model1(data)\n",
    "#         print(data.shape)\n",
    "        data = self.model2(data)\n",
    "#         print(data.shape)\n",
    "        data = self.model3(data)\n",
    "#         print(data.shape)\n",
    "#         data = self.model4(data)\n",
    "#         print(data.shape)\n",
    "#         output = torch.norm(data,dim=-1,keepdim=True).squeeze().squeeze()\n",
    "#         print('encoder output.shape',output.shape)\n",
    "        return data\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self,l0=16,l1=512,l2=4024,l3=902629):\n",
    "#         super(Decoder,self).__init__()\n",
    "#         self.fc1=nn.Linear(l0,l1)\n",
    "#         self.fc2=nn.Linear(l1,l2)\n",
    "#         self.fc3=nn.Linear(l2,l3)\n",
    "# #         self.fc4=nn.Linear(l3,l4)\n",
    "\n",
    "#     def forward(self,x):\n",
    "# #         print(x.shape)\n",
    "#         x = self.fc1(x)\n",
    "# #         print(x.shape)\n",
    "#         x=f.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x=f.relu(x)\n",
    "# #         print(x.shape)\n",
    "#         x=self.fc3(x)\n",
    "# #         x=f.relu(x)\n",
    "# #         x=self.fc4(x)\n",
    "#         x=torch.sigmoid(x)\n",
    "# #         print(x.shape)\n",
    "#         return x\n",
    "\n",
    "class CapsNet3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsNet3D,self).__init__()\n",
    "        self.enc = Encoder()\n",
    "#         self.dec = Decoder()\n",
    "        self.encloss=nn.BCELoss()\n",
    "#         self.encloss=nn.CrossEntropyLoss()\n",
    "    def margin_loss(self,pred_prob,target):\n",
    "\n",
    "#         print('pred_prob.shape',pred_prob.shape)\n",
    "#         print('pred_prob[:5]',pred_prob[:5])\n",
    "#         print('target.shape',target.shape)\n",
    "#         print('target[:5]',target[:5])\n",
    "        \n",
    "        _, max_length_indices = pred_prob.max(dim=1)\n",
    "#         print('_.shape',_.shape)\n",
    "#         print('_[:5]',_[:5])\n",
    "#         print('max_length_indices.shape',max_length_indices.shape)\n",
    "#         print('max_length_indices[:5]',max_length_indices[:5])\n",
    "        masked= Variable(torch.sparse.torch.eye(n_classes_by_val),requires_grad=False)\n",
    "#         masked = torch.eye(n_classes)\n",
    "        if USE_CUDA:\n",
    "            masked=masked.cuda()\n",
    "        \n",
    "#         print('masked.shape',masked.shape)\n",
    "#         print('masked',masked)\n",
    "#         masked = masked.index_select(dim=0, index=max_length_indices.data)\n",
    "        masked = masked.index_select(dim=0, index=target)\n",
    "\n",
    "        Tk = masked\n",
    "#         print(\"Tk.shape\",Tk.shape)\n",
    "#         print(\"Tk[0:5]\",Tk[0:5])\n",
    "        left = Tk*F.relu(0.9-pred_prob)\n",
    "#         print('left.shape',left.shape)\n",
    "        right = 0.5*(1-Tk)*F.relu(pred_prob-0.1)\n",
    "#         print('right.shape',right.shape)\n",
    "        total =(left+right).mean()\n",
    "#         print('total.shape',total.shape)\n",
    "        return total\n",
    "    def loss(self,pred_prob,target_class,target_hot, target_recon=None,decout=None):\n",
    "#         pred_prob =torch.norm(encout,dim=-1,keepdim=False)\n",
    "        encoder_loss = self.encloss(pred_prob,target_hot)\n",
    "#         encoder_loss = self.encloss(pred_prob,target_class)\n",
    "#         print(target_class[0])\n",
    "#         print(pred_prob[0])\n",
    "        margin = self.margin_loss(pred_prob,target_class)\n",
    "#         decoder_loss = ((target_recon-decout)**2).sum()\n",
    "#         total = margin\n",
    "#         total = encoder_loss,margin\n",
    "#         print(total, typeof)\n",
    "#         return encoder_loss,decoder_loss\n",
    "#         return margin,decoder_loss\n",
    "#         return encoder_loss\n",
    "#         return margin\n",
    "        return margin+encoder_loss\n",
    "    \n",
    "    def fetch(self,encout):\n",
    "#         print('fetching')\n",
    "#         print('fetch encout.shape',encout.shape)\n",
    "        if len(encout.shape)==2:\n",
    "            encout=encout.unsqueeze(0)\n",
    "        pred_prob =torch.norm(encout,dim=-1,keepdim=False)#.squeeze()\n",
    "#         print(' fetch prob',pred_prob.shape)\n",
    "        if len(pred_prob.shape)==1:\n",
    "            pred_prob=pred_prob.reshape(1,-1)\n",
    "        _, max_length_indices = pred_prob.max(dim=1)\n",
    "#         print('_.shape',_.shape)\n",
    "#         print('_[:10]',_[:10])\n",
    "#         print('max_length_indices.shape',max_length_indices.shape)\n",
    "#         print('max_length_indices[:10]',max_length_indices[:10])\n",
    "        masked= Variable(torch.sparse.torch.eye(n_classes_by_val),requires_grad=False)\n",
    "\n",
    "#         masked = torch.sparse.torch.eye(n_classes)\n",
    "        if USE_CUDA:\n",
    "            masked=masked.cuda()\n",
    "        \n",
    "#         print('masked.shape',masked.shape)\n",
    "#         print('masked',masked)\n",
    "        masked = masked.index_select(dim=0, index=max_length_indices.data)\n",
    "#         print('masked.shape',masked.shape)\n",
    "#         print('masked',masked)\n",
    "#         print('encout.shape',encout.shape)\n",
    "\n",
    "        decin =torch.matmul(masked[:, None,:],encout)\n",
    "#         print('decin.shape',decin.shape)\n",
    "        decin = decin.view(encout.size(0), -1)\n",
    "#         print('decin',decin)\n",
    "#         print(decin[0,:])\n",
    "#         print(encout[0,max_length_indices[0],:])\n",
    "        return pred_prob,decin\n",
    "        \n",
    "    def forward(self,x):\n",
    "        encout=self.enc(x)\n",
    "        pred_prob,decin=self.fetch(encout)\n",
    "#         decout=self.dec(decin).reshape(-1,1,28,28)\n",
    "#         print(decout.shape)\n",
    "#         return pred_prob,decout\n",
    "        return pred_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = CapsNet3D().cuda()\n",
    "loss_fn=model.loss\n",
    "# loss_fn = nn.SoftMarginLoss()\n",
    "# loss_fn = nn.BCELoss()\n",
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "# subjZ=[0]*n_classes_by_val\n",
    "\n",
    "# for i,v in enumerate(diagnos.values()):\n",
    "#     subjZ[v]+=n_subj_train[i]\n",
    "# weights = [1/i for i in subjZ]\n",
    "# norm = 0\n",
    "# for i in weights:\n",
    "#     norm+=i**2\n",
    "# norm = norm**0.5\n",
    "# weights=[i/(norm) for i in weights]\n",
    "\n",
    "# loss_fn=nn.CrossEntropyLoss(weight=torch.tensor(weights).cuda())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
