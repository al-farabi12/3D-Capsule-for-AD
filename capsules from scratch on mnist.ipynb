{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as F\n",
    "import torch.nn.functional as f\n",
    "from torch.autograd import Variable\n",
    "import datetime\n",
    "n_classes=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(os.getcwd(),train=True,transform=F.to_tensor)\n",
    "testset = torchvision.datasets.MNIST(os.getcwd(),train=False,transform=F.to_tensor)\n",
    "# trainset = torchvision.datasets.CIFAR10(os.getcwd(),download=True,train=True,transform=F.to_tensor)\n",
    "# cifar_testset = torchvision.datasets.CIFAR10(os.getcwd(),download=True,train=False,transform=F.to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=trainset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "\n",
    "    shuffle=True,)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=testset,\n",
    "                batch_size=batch_size,\n",
    "                drop_last=True,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self,in_ch=1,out_ch=256,kernel_size=9,stride=1):\n",
    "        super(ConvLayer,self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_ch,out_ch,kernel_size,stride)\n",
    "    def squash(self,x):\n",
    "        norms = torch.norm(x,dim=1,keepdim=True)\n",
    "        x = ((norms/(1+norms**2))*x)\n",
    "        return x\n",
    "    def forward(self,x):\n",
    "#         print(x.shape)\n",
    "        x = self.squash(self.conv1(x))\n",
    "        return x\n",
    "    \n",
    "class PrimaryCapsules(nn.Module):\n",
    "    def __init__(self,in_ch=256,out_caps_grids=32,out_ch_of_each_caps=8,kernel_size=9,stride=2):\n",
    "        super(PrimaryCapsules,self).__init__()\n",
    "        self.out_caps_grids=out_caps_grids\n",
    "        self.out_ch_of_each_caps=out_ch_of_each_caps\n",
    "        self.prim_caps = torch.nn.Conv2d(in_ch,out_caps_grids*out_ch_of_each_caps,kernel_size,stride)\n",
    "    def reshape(self,x):\n",
    "        b,d1,d2,d3=x.shape\n",
    "        x=x.permute(0,2,3,1)\n",
    "        x=x.reshape(b,d2,d3,self.out_caps_grids,self.out_ch_of_each_caps)\n",
    "        return x\n",
    "    def squash(self,x):\n",
    "        norms = torch.norm(x,dim=-1,keepdim=True)\n",
    "        x = ((norms/(1+norms**2))*x)\n",
    "        return x\n",
    "    def forward(self,x):\n",
    "        return self.squash(self.reshape(self.prim_caps(x)))\n",
    "    \n",
    "class DigitCaps(nn.Module):\n",
    "    def __init__(self,input_grid=( 6, 6, 32),in_cap_dim=8,out_cap_dim=16,out_caps=10):\n",
    "        super(DigitCaps,self).__init__()\n",
    "        self.in_cap_dim=in_cap_dim\n",
    "        self.num_caps = 1\n",
    "        \n",
    "        for i in input_grid:\n",
    "            self.num_caps*=i\n",
    "        self.W = torch.nn.Parameter(torch.randn(1, out_caps, self.num_caps, out_cap_dim, in_cap_dim))\n",
    "        self.b = torch.nn.Parameter(torch.zeros(1, out_caps, self.num_caps,1,1),requires_grad=False)\n",
    "    def reshape(self,x):\n",
    "        batch_size=x.shape[0]\n",
    "        x=x.reshape(batch_size,1,-1,self.in_cap_dim,1)\n",
    "        return x\n",
    "    def squash(self,x):\n",
    "        norms = torch.norm(x,dim=-1,keepdim=True)\n",
    "        x = ((norms/(1+norms**2))*x)\n",
    "        return x\n",
    "    def forward(self,x):\n",
    "        batch_size=x.shape[0]\n",
    "#         print('x',x.shape)\n",
    "        u = self.reshape(x)\n",
    "#         print('w',self.W.shape)\n",
    "#         print('u',u.shape)\n",
    "        \n",
    "        uhat = torch.matmul(self.W,u)\n",
    "#         print('uhat',uhat.shape)\n",
    "        b = self.b\n",
    "        for i in range(3):\n",
    "#             print('iter ',i)\n",
    "#             print('b',self.b.shape)\n",
    "            c = torch.softmax(b,dim=2)\n",
    "#             print('c',c.shape)\n",
    "#             print(c.sum())\n",
    "#             try:\n",
    "            ahat= (uhat*c)\n",
    "#             except:\n",
    "#                 print('b',self.b.shape)\n",
    "\n",
    "#                 print('uhat',uhat.shape)\n",
    "#                 print('c',c.shape)\n",
    "\n",
    "#             print('ahat',ahat.shape)\n",
    "            a=ahat.sum(dim=2,keepdim=True).permute(0,1,2,4,3)\n",
    "#             print('a',a.shape)\n",
    "#             print('uhat',uhat.shape)\n",
    "\n",
    "            adotuhat= torch.matmul(a,uhat).mean(dim=0,keepdim=True)\n",
    "#             print('adotuhat.shape',adotuhat.shape)\n",
    "#             print('b.shape',b.shape)\n",
    "            b = b+adotuhat\n",
    "            if self.training:\n",
    "                self.b=torch.nn.Parameter(b,requires_grad=False)\n",
    "        x = self.squash(a)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = ConvLayer()\n",
    "# model2 = PrimaryCapsules()\n",
    "# model3 = DigitCaps()\n",
    "# for batch_id, (data, target) in enumerate(train_loader,1):\n",
    "#     data = model1(data)\n",
    "#     print(data.shape)\n",
    "#     data = model2(data)\n",
    "#     print(data.shape)\n",
    "#     data = model3(data)\n",
    "#     print(data.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.l1=ConvLayer()\n",
    "        self.l2=PrimaryCapsules()\n",
    "        self.l3=DigitCaps()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x= self.l1(x)\n",
    "#         print(x.shape)\n",
    "        x=self.l2(x)\n",
    "#         print(x.shape)\n",
    "        x=self.l3(x)\n",
    "#         print(x.shape)\n",
    "        \n",
    "#         print(output.shape)\n",
    "        x = x.squeeze().squeeze()\n",
    "#         print('x.shape',x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,l0=16,l1=512,l2=1024,l3=784):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.fc1=nn.Linear(l0,l1)\n",
    "        self.fc2=nn.Linear(l1,l2)\n",
    "        self.fc3=nn.Linear(l2,l3)\n",
    "\n",
    "    def forward(self,x):\n",
    "#         print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "#         print(x.shape)\n",
    "        x=f.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x=f.relu(x)\n",
    "#         print(x.shape)\n",
    "        x=self.fc3(x)\n",
    "        x=torch.sigmoid(x)\n",
    "#         print(x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsNet3D,self).__init__()\n",
    "        self.enc = Encoder()\n",
    "        self.dec = Decoder()\n",
    "        self.encloss=nn.CrossEntropyLoss()\n",
    "    def margin_loss(self,pred_prob,target):\n",
    "\n",
    "#         print('pred_prob.shape',pred_prob.shape)\n",
    "#         print('pred_prob[:5]',pred_prob[:5])\n",
    "#         print('target.shape',target.shape)\n",
    "#         print('target[:5]',target[:5])\n",
    "        \n",
    "        _, max_length_indices = pred_prob.max(dim=1)\n",
    "#         print('_.shape',_.shape)\n",
    "#         print('_[:5]',_[:5])\n",
    "#         print('max_length_indices.shape',max_length_indices.shape)\n",
    "#         print('max_length_indices[:5]',max_length_indices[:5])\n",
    "        masked= Variable(torch.sparse.torch.eye(n_classes),requires_grad=False)\n",
    "#         masked = torch.eye(n_classes)\n",
    "        if USE_CUDA:\n",
    "            masked=masked.cuda()\n",
    "        \n",
    "#         print('masked.shape',masked.shape)\n",
    "#         print('masked',masked)\n",
    "#         masked = masked.index_select(dim=0, index=max_length_indices.data)\n",
    "        masked = masked.index_select(dim=0, index=target)\n",
    "\n",
    "        Tk = masked\n",
    "#         print(\"Tk.shape\",Tk.shape)\n",
    "#         print(\"Tk[0:5]\",Tk[0:5])\n",
    "        left = Tk*f.relu(0.9-pred_prob)**2\n",
    "#         print('left.shape',left.shape)\n",
    "        right = 0.5*(1-Tk)*f.relu(pred_prob-0.1)**2\n",
    "#         print('right.shape',right.shape)\n",
    "        total =(left+right).sum()\n",
    "#         print('total.shape',total.shape)\n",
    "        return total\n",
    "    def loss(self,pred_prob,target_class,target_recon=None,decout=None):\n",
    "#         pred_prob =torch.norm(encout,dim=-1,keepdim=False)\n",
    "#         encoder_loss = self.encloss(pred_prob,target_class)\n",
    "#         print(target_class[0])\n",
    "#         print(pred_prob[0])\n",
    "        margin = self.margin_loss(pred_prob,target_class)\n",
    "        decoder_loss = ((target_recon-decout)**2).sum()\n",
    "#         total = margin\n",
    "#         total = encoder_loss,margin\n",
    "#         print(total, typeof)\n",
    "#         return encoder_loss,decoder_loss\n",
    "        return margin,decoder_loss\n",
    "    \n",
    "    def fetch(self,encout):\n",
    "#         print('fetching')\n",
    "# #         print('encout.shape',encout.shape)\n",
    "        pred_prob =torch.norm(encout,dim=-1,keepdim=False)#.squeeze()\n",
    "#         print('prob',pred_prob.shape)\n",
    "        \n",
    "        _, max_length_indices = pred_prob.max(dim=1)\n",
    "#         print('_.shape',_.shape)\n",
    "#         print('_[:10]',_[:10])\n",
    "#         print('max_length_indices.shape',max_length_indices.shape)\n",
    "#         print('max_length_indices[:10]',max_length_indices[:10])\n",
    "        masked= Variable(torch.sparse.torch.eye(n_classes),requires_grad=False)\n",
    "\n",
    "#         masked = torch.sparse.torch.eye(n_classes)\n",
    "        if USE_CUDA:\n",
    "            masked=masked.cuda()\n",
    "        \n",
    "#         print('masked.shape',masked.shape)\n",
    "#         print('masked',masked)\n",
    "        masked = masked.index_select(dim=0, index=max_length_indices.data)\n",
    "#         print('masked.shape',masked.shape)\n",
    "#         print('masked',masked)\n",
    "#         print('encout.shape',encout.shape)\n",
    "\n",
    "        decin =torch.matmul(masked[:, None,:],encout).view(encout.size(0), -1)\n",
    "#         print('decin',decin)\n",
    "#         print('decin.shape',decin.shape)\n",
    "#         print(decin[0,:])\n",
    "#         print(encout[0,max_length_indices[0],:])\n",
    "        return pred_prob,decin\n",
    "        \n",
    "    def forward(self,x):\n",
    "        encout=self.enc(x)\n",
    "        pred_prob,decin=self.fetch(encout)\n",
    "        decout=self.dec(decin).reshape(-1,1,28,28)\n",
    "#         print(decout.shape)\n",
    "        return pred_prob,decout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps=CapsNet3D().cuda()\n",
    "# model =caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=caps.loss\n",
    "model = nn.DataParallel(caps)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): CapsNet3D(\n",
      "    (enc): Encoder(\n",
      "      (l1): ConvLayer(\n",
      "        (conv1): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
      "      )\n",
      "      (l2): PrimaryCapsules(\n",
      "        (prim_caps): Conv2d(256, 256, kernel_size=(9, 9), stride=(2, 2))\n",
      "      )\n",
      "      (l3): DigitCaps()\n",
      "    )\n",
      "    (dec): Decoder(\n",
      "      (fc1): Linear(in_features=16, out_features=512, bias=True)\n",
      "      (fc2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (fc3): Linear(in_features=1024, out_features=784, bias=True)\n",
      "    )\n",
      "    (encloss): CrossEntropyLoss()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8141840"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch/n_epochs 0 / 100\n",
      "870 / 30000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9343169a8ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mtrain_loss_enc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mtrain_loss_dec\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m#         print(output.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k1=1\n",
    "k2=0\n",
    "USE_CUDA=True\n",
    "n_epochs = 100\n",
    "start = 0\n",
    "l = 3\n",
    "train_loss_accuracy_hist=np.zeros((l,n_epochs))\n",
    "test_loss_accuracy_hist=np.zeros((l,n_epochs))\n",
    "ltr= len(train_loader)\n",
    "lts= len(test_loader)\n",
    "for epoch in range(start,start+n_epochs):\n",
    "    print('epoch/n_epochs',epoch,'/',n_epochs)\n",
    "    model.train()\n",
    "    train_loss_enc = 0\n",
    "    train_loss_dec = 0\n",
    "    train_pred = np.empty((batch_size), int)\n",
    "    train_targets = np.empty((batch_size), int)\n",
    "    test_targets = np.empty((batch_size), int)\n",
    "    test_pred =  np.empty((batch_size), int)\n",
    "    for batch_id, (data, target) in enumerate(train_loader,1):\n",
    "#         target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n",
    "        print(batch_id,'/',ltr,end='\\r')\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        if USE_CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "#         pred_prob = model(data)\n",
    "        pred_prob,dec = model(data)\n",
    "\n",
    "        loss1,loss2=loss_fn(pred_prob,target,data,dec)\n",
    "#         loss=loss_fn(pred_prob,target,data,None)\n",
    "#         loss1,loss2=loss_fn(pred_prob,target)\n",
    "#         print(loss1.item(),loss2.item())\n",
    "#         loss = loss1#+loss2\n",
    "        loss1,loss2 = k1*loss1,k2*loss2 \n",
    "        loss = loss1+loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_enc += loss1.item()\n",
    "        train_loss_dec += loss2.item()\n",
    "#         print(output.shape)\n",
    "        train_pred = np.append(train_pred, np.argmax(pred_prob.data.cpu().numpy(), 1), axis=0)\n",
    "        train_targets = np.append(train_targets, target.cpu(), axis=0)\n",
    "#         print(pre1,pre2)\n",
    "#         if batch_id % 1 == 0:\n",
    "#             print('epoch',epoch, 'batch_id',batch_id,\"train accuracy:\", sum(train_pred[batch_size:] == train_targets[batch_size:]) / len(train_pred[batch_size:]),end='\\r')\n",
    "#         if batch_id == 1:\n",
    "#             break\n",
    "    train_targets=train_targets[batch_size:]\n",
    "    train_pred=train_pred[batch_size:]\n",
    "    train_loss_accuracy_hist[:,epoch]=[train_loss_enc/ len(train_loader),train_loss_dec / len(train_loader),sum(train_pred == train_targets) / len(train_pred)]\n",
    "\n",
    "    \n",
    "    print ('\\ntrain loss enc',train_loss_accuracy_hist[0,epoch],'train loss dec',train_loss_accuracy_hist[1,epoch],'\\ntrain accuracy',train_loss_accuracy_hist[-1,epoch],\"\\n\")\n",
    "#     if epoch == 0:\n",
    "#         break\n",
    "\n",
    "    model.eval()\n",
    "    test_loss_enc = 0\n",
    "    test_loss_dec = 0\n",
    "    inxHistTest=[]\n",
    "    for batch_id, (data, target) in enumerate(test_loader,1):\n",
    "#         target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n",
    "        print('\\t\\t\\t',batch_id,'/',lts,end='\\r')\n",
    "\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "#         pred_prob = model(data)\n",
    "        pred_prob,dec = model(data)\n",
    "#         loss=loss_fn(target,pred_prob,data,None)\n",
    "        loss1,loss2=loss_fn(pred_prob,target,data,dec)\n",
    "#         print(loss1.item(),loss2.item())\n",
    "        loss1,loss2 = k1*loss1,k2*loss2\n",
    "#         loss=loss_fn(pred_prob,target,data,dec)\n",
    "        loss = loss1+loss2\n",
    "#         loss = loss2\n",
    "\n",
    "        test_loss_enc += loss1.item()\n",
    "        test_loss_dec += loss2.item()\n",
    "\n",
    "#         print(test_pred.shape)\n",
    "        test_pred = np.append(test_pred, np.argmax(pred_prob.data.cpu().numpy(), 1), axis=0)\n",
    "#         print('test_pred',test_pred)\n",
    "#         test_targets = np.append(test_targets, np.argmax(target.data.cpu().numpy(), 1), axis=0)\n",
    "        test_targets = np.append(test_targets, target.cpu(), axis=0)\n",
    "#         print('test_targets',test_targets)\n",
    "#         if batch_id % 1 == 0:\n",
    "#             print (\"\\t\\t\\t\",'epoch',epoch, 'batch_id',batch_id,\"test accuracy:\", sum(test_pred[batch_size:] == test_targets[batch_size:]) / len(test_pred[batch_size:]),end='\\r')\n",
    "#         if batch_id % 3 == 0:\n",
    "#             break\n",
    "    test_targets=test_targets[batch_size:]\n",
    "    test_pred=test_pred[batch_size:]\n",
    "    test_loss_accuracy_hist[:,epoch]=[test_loss_enc/ len(test_loader),test_loss_dec / len(test_loader),sum(test_pred == test_targets) / len(test_pred)]\n",
    "#     print('test_targets',test_targets,'test_pred',test_pred)\n",
    "    print ('\\n\\t\\t\\ttest loss enc',test_loss_accuracy_hist[0,epoch],'test loss dec',test_loss_accuracy_hist[1,epoch],'\\n\\t\\t\\ttest accuracy',test_loss_accuracy_hist[-1,epoch],\"\\n\")\n",
    "#     if epoch==1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "# n_epochs=100\n",
    "rang = list(range(n_epochs))\n",
    "\n",
    "plt.subplot(131)\n",
    "# plt.ylim(-0.1, 1.1)\n",
    "# plt.xlim(0,1.1)\n",
    "plt.plot(rang,train_loss_accuracy_hist[0,:n_epochs],label='train')\n",
    "plt.plot(rang,test_loss_accuracy_hist[0,:n_epochs],label='test')\n",
    "plt.tick_params(axis='y', which='both', labelleft='on', labelright='on')\n",
    "\n",
    "plt.title(\"Encoder Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(132)\n",
    "# plt.ylim(-0.1, 1.1)\n",
    "# plt.xlim(0,1.1)\n",
    "plt.plot(rang,train_loss_accuracy_hist[1,:n_epochs],label='train')\n",
    "plt.plot(rang,test_loss_accuracy_hist[1,:n_epochs],label='test')\n",
    "plt.title(\"Decoder Loss\")\n",
    "plt.tick_params(axis='y', which='both', labelleft='on', labelright='on')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "# plt.xlim(0,1)\n",
    "plt.plot(rang,train_loss_accuracy_hist[-1,:n_epochs],label='train')\n",
    "plt.plot(rang,test_loss_accuracy_hist[-1,:n_epochs],label='test')\n",
    "plt.tick_params(axis='y', which='both', labelleft='on', labelright='on')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "now = datetime.datetime.now()\n",
    "fname= f\"results/{now.year}-{now.month}-{now.day}-{now.hour}-{now.minute}-results-mnist.png\"\n",
    "# plt.savefig(fname)\n",
    "plt.show()\n",
    "print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info =str(model)+'\\n'+str(optimizer)\n",
    "file = open(f\"results/{now.year}-{now.month}-{now.day}-{now.hour}-{now.minute}-{params}-info-mnist.txt\",\"w+\")\n",
    "file.write(info)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA=True\n",
    "n_epochs = 1\n",
    "start = 0\n",
    "for epoch in range(start,start+n_epochs):\n",
    "    model.eval()\n",
    "    for batch_id, (data, target) in enumerate(test_loader,1):\n",
    "#         target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n",
    "        print('\\t\\t\\t',batch_id,'/',lts,end='\\r')\n",
    "\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "#         pred_prob = model(data)\n",
    "        pred_prob,dec = model(data)\n",
    "        print(pred_prob.shape)\n",
    "        aa = pred_prob.max(dim=1)\n",
    "        print(aa.indices.shape)\n",
    "        print(target.shape)\n",
    "        comp = aa.indices==target\n",
    "#         print(comp)\n",
    "        for idx,i in enumerate(comp):\n",
    "            if not i:\n",
    "                print('target ',target[idx].item(),' prediction ',aa.indices[idx].item())\n",
    "                r = idx\n",
    "\n",
    "                plt.imshow(data.cpu()[r,0])\n",
    "                plt.show()\n",
    "                plt.imshow(dec.detach().cpu().numpy()[r,0])\n",
    "                plt.show()\n",
    "#             if idx ==100:\n",
    "#                 break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
